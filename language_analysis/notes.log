
<date>20150618</date>

https://en.wikipedia.org/wiki/List_of_text_corpora

http://corpus.byu.edu/full-text/

http://linguistics.stackexchange.com/questions/4232/english-text-corpus-for-download

http://www.linguistics.ucsb.edu/research/santa-barbara-corpus

**********************

NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources 
http://www.nltk.org/

sudo pip install -U nltk
sudo pip install -U numpy

http://www.nltk.org/data.html

>>> import nltk
>>> nltk.download()

<date>20150614</date>

How to solve word logic problems?

There is a spectrum of structure in text

continuous noise -- discrete noise -- symbols -- random letters -- random words -- random sentences -- "text" -- structured text

What distinguishes one location on the spectrum from another?

Asking about the computational complexity of word logic problems doesn't capture the issue of relations necessary to form information

Computational complexity is about comparing the count of steps. 

does Shannon entropy cover sufficiently describe structure? I think Shannon entropy is useful for analyzing (data --> information)
What is the measure of (information --> knowledge)?
I think analyzing the combinatorial explosion associated with candidate letters and words is not productive
Analyzing the rules associated with text processing only gets so far, as the rule are more like suggestions

******************

In measuring shannon entropy, what is the variance for a fixed size of text?
For each sample, measure Shannon entropy (s). Then for 100 samples (n), histogram the values. 
claim: The variance of the histogram should decrease as sample size (k) increases

* 100 samples of 1000 letters
* 100 samples of 10000 letters
* 100 samples of 100000 letters

* 100 samples of 100 words
* 100 samples of 1000 words
* 100 samples of 10000 words

How diverse should the sample sources be?

Does variance simply indicate sample size is too small?

*************************

Testing humans for cognition is easy because the tests are created by humans with human-curated knowledge.
Example: given 20 words, group by topic
A computer, lacking the experience which builds necessary knowledge, would need a larger training set


Then either
* how to build a knowledge base from experience
* can knowledge be created without the investment in creating a knowledge base

What is a knowledgebase? (a basis on which to be knowledgable)

claim: Knowledge is the connections between information. 
if the claim is valid, then to create knowledge is a 2 step process:
1) gather information
2) create connections
However, merely identifying connections does not make the knowledgebase useful for queries
A knowledgebase also requires an ontology, giving structure to the relations (rather than randomly labeled relations)

How does an ontology make a knowledgebase more useful for queries?

************************

Shannon entropy measures information complexity 
What measures information density? 
